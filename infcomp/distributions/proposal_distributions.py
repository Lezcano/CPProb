import math
import torch
import torch.nn as nn
import numpy as np
from infcomp.util import logsumexp

import infcomp.protocol
# They are used via getattr, even though some IDEs might not detect this
from infcomp.protocol import (Normal,
                              Mixture,
                              Truncated,
                              Discrete,
                              TableDistribution)
from infcomp.protocol.Distribution import Distribution

from infcomp.distributions.projections import (RealProjection,
                                               RealPositiveProjection,
                                               ScalarHyperparamType,
                                               LogSimplexProjection,
                                               ProjectionList,
                                               DistributionProjection)


def _unpack_scalar_values(params):
    return map(lambda x: x.value, params)


class ProposalDistribution(nn.Module):
    def __init__(self, name, params, hyperparams, input_dim):
        super().__init__()
        self._name = name
        self._input_dim = input_dim
        self._params = nn.ModuleList(params)
        # Hyperparams are not projected, so they are not modules
        self._hyperparams = hyperparams

    def forward(self, proposal_input):
        assert proposal_input.size(1) == self._input_dim
        return [param(proposal_input) for param in self._params]

    def serialize(self, builder, parameters):
        mod = getattr(infcomp.protocol, self._name)
        # The vectorial parameters have to be packed before starting to pack the distribution
        packed_params = [param.pack_value(builder=builder, mod=mod, distr_name=self._name, value=value)
                         for param, value in zip(self._params, parameters)]

        packed_hyperparams = [hyperparam.pack_value(builder=builder, mod=mod, distr_name=self._name)
                              for hyperparam in self._hyperparams]
        getattr(mod, self._name + "Start")(builder)
        for param, value in zip(self._params, packed_params):
            param.pack(builder=builder, mod=mod, distr_name=self._name, value=value)
        for hyperparam, value in zip(self._hyperparams, packed_hyperparams):
            hyperparam.pack(builder=builder, mod=mod, distr_name=self._name, value=value)
        distribution = getattr(mod, self._name + "End")(builder)
        distribution_type = getattr(Distribution(), self._name)
        return distribution, distribution_type

    def log_pdf(self, *args):
        raise NotImplementedError


class ProposalNormal(ProposalDistribution):
    def __init__(self, distribution_fbb, input_dim):
        # The name has to be the name of the object generated by flatbuffers
        name = "Normal"
        # The order in params and in the unpacking in log_pdf have to be in the same
        params = [
            RealProjection(name="Mean", input_dim=input_dim),
            RealPositiveProjection(name="Std", input_dim=input_dim)
        ]
        super().__init__(name=name, params=params, hyperparams=[], input_dim=input_dim)

    def log_pdf(self, x, mu, sigma):
        return -1 * (torch.log(sigma) + 0.5 * np.log(2.0 * np.pi) + 0.5 * torch.pow((x - mu) / sigma, 2))

    def cdf(self, x, mu, sigma):
        return 0.5 * (1 + torch.erf((x - mu) / (sigma*np.sqrt(2.0))))


class ProposalDiscrete(ProposalDistribution):
    def __init__(self, distribution_fbb, input_dim):
        # The name has to be the name of the object generated by flatbuffers
        name = "Discrete"
        hyperparams = [
            ScalarHyperparamType(name="Min", distribution_fbb=distribution_fbb),
            ScalarHyperparamType(name="Max", distribution_fbb=distribution_fbb)
        ]
        a, b = _unpack_scalar_values(hyperparams)
        params = [
            LogSimplexProjection(name="Probabilities", input_dim=input_dim, dim=b - a + 1, exponentiate=True)
        ]
        super().__init__(name=name, params=params, hyperparams=hyperparams, input_dim=input_dim)

    def log_pdf(self, x, logits):
        a, b = _unpack_scalar_values(self._hyperparams)
        if logits.size(1) != b - a + 1:
            raise ValueError("Embedding length {} doesn't match with the proposed embedding {}.".format(logits.size(1), b[0] - a[0] + 1))

        idx = x-a
        if (idx.gt(b - a) + idx.lt(0)).any():
            raise ValueError("Index out of range.")

        idx = idx.unsqueeze(1)
        return logits.gather(1, idx)


class ProposalMixture(ProposalDistribution):
    def __init__(self, distribution_fbb, input_dim, distribution_type, n):
        name = "Mixture"
        params = [
            LogSimplexProjection(name="Coefficients", input_dim=input_dim, dim=n, exponentiate=True),
            ProjectionList(name="Distributions", projections=[DistributionProjection(name="",
                                                                                     input_dim=input_dim,
                                                                                     distribution_type=distribution_type,
                                                                                     distribution_fbb=distribution_fbb)
                                                              for _ in range(n)])
        ]
        super().__init__(name=name, params=params, hyperparams=[], input_dim=input_dim)

    def log_pdf(self, x, logits, *distributions_params):
        logpdfs = self._params[1].log_pdf(x, *distributions_params)
        if logits.size() != logpdfs.size():
            raise ValueError("Sizes logpdfs {} and logits {} do not agree".format(logpdfs.size(), logits.size()))
        # Discard components of the mixture if needed
        # Cut backprop for nodes where p_i < e^-8
        threshold = -8
        logits_clamped = torch.clamp(logits, min=threshold)
        logits_clamped.masked_fill_(logits_clamped.eq(threshold), float("-inf"))
        return logsumexp(logits_clamped + logpdfs, dim=1)

    def cdf(self, x, logits, *distributions_params):
        cdfs = self._params[1].cdf(x, *distributions_params)
        if logits.size() != cdfs.size():
            raise ValueError("Sizes cdfs {} and logits {} do not agree".format(cdfs.size(), logits.size()))
        # Discard components of the mixture if needed
        # Cut backprop for nodes where p_i < e^-8
        threshold = -8
        logits_clamped = torch.clamp(logits, threshold)
        logits_clamped.masked_fill_(logits_clamped.eq(threshold), float("-inf"))
        return torch.sum(torch.mul(logits_clamped.exp(), cdfs), dim=1)


def mixture_builder(distribution_type, n):
    def wrapper(distribution_fbb, input_dim):
        return ProposalMixture(distribution_fbb=distribution_fbb,
                               input_dim=input_dim,
                               distribution_type=distribution_type,
                               n=n)
    return wrapper


class ProposalTruncated(ProposalDistribution):
    def __init__(self, distribution_fbb, input_dim, distribution_type):
        name = "Truncated"
        hyperparams = [
            ScalarHyperparamType(name="Min", distribution_fbb=distribution_fbb),
            ScalarHyperparamType(name="Max", distribution_fbb=distribution_fbb)
        ]
        params = [
            DistributionProjection(name="Distribution",
                                   input_dim=input_dim,
                                   distribution_type=distribution_type,
                                   distribution_fbb=distribution_fbb)
        ]
        super().__init__(name=name, params=params, hyperparams=hyperparams, input_dim=input_dim)

    def log_pdf(self, x, distribution_params):
        a, b = _unpack_scalar_values(self._hyperparams)
        distr = self._params[0]
        logpdf = distr.log_pdf(x, *distribution_params)
        cdf = distr.cdf(b, *distribution_params) - distr.cdf(a, *distribution_params)
        # Discard components of the mixture if needed
        # Cut backprop for nodes where cdf = 0
        mask = cdf.eq(0)
        cdf.masked_fill_(mask, 1)
        logpdf.masked_fill_(mask, -math.log(b-a))
        return logpdf - torch.log(cdf)


def truncated_builder(distribution_type):
    return lambda distribution_fbb, input_dim: ProposalTruncated(distribution_fbb=distribution_fbb,
                                                                 input_dim=input_dim,
                                                                 distribution_type=distribution_type)


class ProposalProduct(ProposalDistribution):
    def __init__(self, distribution_fbb, input_dim, distribution_type, n):
        name = "Product"
        params = [
            ProjectionList(name="Distributions", projections=[DistributionProjection(name="",
                                                                                     input_dim=input_dim,
                                                                                     distribution_type=distribution_type,
                                                                                     distribution_fbb=distribution_fbb)
                                                              for _ in range(n)])
        ]
        super().__init__(name=name, params=params, hyperparams=[], input_dim=input_dim)

    def log_pdf(self, x, *distributions_params):
        return torch.sum(self._params[0].log_pdf(x, *distributions_params), dim=1)
