import torch
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
from infcomp.settings import settings

import infcomp.protocol
# They are used via getattr, even though the IDE does not detect this
from infcomp.protocol import (Normal,
                              Mixture,
                              Truncated,
                              Discrete,
                              TableDistribution)
from infcomp.protocol.Distribution import Distribution

from infcomp.distributions.projections import (RealProjection,
                                               RealPositiveProjection,
                                               ScalarHyperparamType,
                                               SimplexProjection,
                                               ProjectionList,
                                               DistributionProjection)


def _unpack_scalar_values(params):
    return map(lambda x: x.value, params)


class ProposalDistribution(nn.Module):
    def __init__(self, name, params, hyperparams, input_dim):
        super().__init__()
        self._name = name
        self._input_dim = input_dim
        self._params = nn.ModuleList(params)
        # Hyperparams are not projected, so they are not modules
        self._hyperparams = hyperparams

    def forward(self, proposal_input):
        assert proposal_input.size()[1] == self._input_dim
        return [param.forward(proposal_input) for param in self._params]

    def serialize(self, builder, parameters):
        mod = getattr(infcomp.protocol, self._name)
        # The vectorial parameters have to be packed before starting to pack the distribution
        packed_params = [param.pack_value(builder=builder, mod=mod, distr_name=self._name, value=value)
                         for param, value in zip(self._params, parameters)]

        packed_hyperparams = [hyperparam.pack_value(builder=builder, mod=mod, distr_name=self._name)
                              for hyperparam in self._hyperparams]
        getattr(mod, self._name + "Start")(builder)
        for param, value in zip(self._params, packed_params):
            param.pack(builder=builder, mod=mod, distr_name=self._name, value=value)
        for hyperparam, value in zip(self._hyperparams, packed_hyperparams):
            hyperparam.pack(builder=builder, mod=mod, distr_name=self._name, value=value)
        distribution = getattr(mod, self._name + "End")(builder)
        distribution_type = getattr(Distribution(), self._name)
        return distribution, distribution_type

    def log_pdf(self, x, *args):
        raise NotImplemented()


class ProposalNormal(ProposalDistribution):
    def __init__(self, distribution_fbb, input_dim):
        # The name has to be the name of the object generated by flatbuffers
        name = "Normal"
        # The order in params and in the unpacking in log_pdf have to be in the same
        params = [
            RealProjection(name="Mean", input_dim=input_dim),
            RealPositiveProjection(name="Std", input_dim=input_dim)
        ]
        super().__init__(name=name, params=params, hyperparams=[], input_dim=input_dim)

    def log_pdf(self, x, mu, sigma):
        return -1 * (torch.log(sigma) + 0.5 * np.log(2.0 * np.pi) + 0.5 * torch.pow((x - mu) / sigma, 2))

    def cdf(self, x, mu, sigma):
        return 0.5 * (1 + torch.erf((x - mu) / (sigma*np.sqrt(2.0))))


class ProposalDiscrete(ProposalDistribution):
    def __init__(self, distribution_fbb, input_dim):
        # The name has to be the name of the object generated by flatbuffers
        name = "Discrete"
        hyperparams = [
            ScalarHyperparamType(name="Min", distribution_fbb=distribution_fbb),
            ScalarHyperparamType(name="Max", distribution_fbb=distribution_fbb)
        ]
        a, b = _unpack_scalar_values(hyperparams)
        params = [
            SimplexProjection(name="Probabilities", input_dim=input_dim, dim=b - a + 1)
        ]
        super().__init__(name=name, params=params, hyperparams=hyperparams, input_dim=input_dim)

    def log_pdf(self, x, logits):
        a, b = _unpack_scalar_values(self._hyperparams)
        if logits.size(1) != b - a + 1:
            raise ValueError("Embedding length {} doesn't match with the proposed embedding {}.".format(logits.size(1), b[0] - a[0] + 1))
        # Fixme(Lezcano) this does not consider numbers that might be out of the range {a, ... , b}
        # this should be done with torch.masked_select
        # Fixme(Lezcano) Parse integers as LongTensor directly. Same with integer variables
        # Fixme(Lezcano) Change to LogSimplexProjection and send the logits to C++
        return torch.log(torch.gather(logits, 1, (x-Variable(settings.Tensor([a]))).long()))


class ProposalMixture(ProposalDistribution):
    def __init__(self, distribution_fbb, input_dim, distribution_type, n):
        name = "Mixture"
        params = [
            SimplexProjection(name="Coefficients", input_dim=input_dim, dim=n),
            ProjectionList(name="Distributions", projections=[DistributionProjection(name="",
                                                                                     input_dim=input_dim,
                                                                                     distribution_type=distribution_type,
                                                                                     distribution_fbb=distribution_fbb)
                                                              for _ in range(n)])
        ]
        super().__init__(name=name, params=params, hyperparams=[], input_dim=input_dim)

    def log_pdf(self, x, logits, *distributions_params):
        logpdfs = self._params[1].log_pdf(x, *distributions_params)
        if logits.size() != logpdfs.size():
            raise ValueError("Sizes logpdfs {} and logits {} do not agree".format(logpdfs.size(), logits.size()))
        # TODO(Lezcano) Check that this is indeed stable, although I think it should be
        return torch.log(torch.sum(torch.mul(logits, torch.exp(logpdfs)), dim=1))

    def cdf(self, x, logits, *distributions_params):
        cdfs = self._params[1].cdf(x, *distributions_params)
        if logits.size() != cdfs.size():
            raise ValueError("Sizes cdfs {} and logits {} do not agree".format(cdfs.size(), logits.size()))
        return torch.sum(torch.mul(logits, cdfs), dim=1)


class ProposalTruncated(ProposalDistribution):
    def __init__(self, distribution_fbb, input_dim, distribution_type):
        name = "Truncated"
        hyperparams = [
            ScalarHyperparamType(name="Min", distribution_fbb=distribution_fbb),
            ScalarHyperparamType(name="Max", distribution_fbb=distribution_fbb)
        ]
        params = [
            DistributionProjection(name="Distribution",
                                   input_dim=input_dim,
                                   distribution_type=distribution_type,
                                   distribution_fbb=distribution_fbb)
        ]
        super().__init__(name=name, params=params, hyperparams=hyperparams, input_dim=input_dim)

    def log_pdf(self, x, distribution_params):
        a, b = _unpack_scalar_values(self._hyperparams)
        distr = self._params[0]
        return distr.log_pdf(x, *distribution_params) - torch.log((distr.cdf(b, *distribution_params) -
                                                                   distr.cdf(a, *distribution_params)))
